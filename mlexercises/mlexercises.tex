\documentclass[10pt,a4paper]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[parfill]{parskip}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{titling}
\usepackage{titlesec}
\usepackage[pdftex,hidelinks]{hyperref}

% A timesaver
\mathtoolsset{showonlyrefs}

% No section numbering and sparse usage of the toc
\setcounter{secnumdepth}{0}
\setcounter{tocdepth}{2}

%% Defining problems
\newenvironment{prob}[1]%
   {%\addcontentsline{toc}{subsubsection}{#1}%
    \begin{description}\item[#1]}%
   {\end{description}}


\title{Answers to Machine Learning Exercises}
\author{Bj√∂rn Lindqvist\\bjourne@gmail.com}
\date{\today}

% Document title
\makeatletter
\hypersetup{
    pdftitle = {\@title},
    pdfauthor = {\@author}
}
\makeatother

\begin{document}
\begin{titlingpage}
  \maketitle
  \thispagestyle{empty}
  \section*{About}
  \LaTeX solutions to exercises in machine learning.
  \tableofcontents
\end{titlingpage}

\section{Pattern Recognition and Machine Learning}

\subsection{Chapter 2}
\begin{prob}{2.1}
  Show that the Bernoulli distribution satisfies the following
  properties
  \begin{align}
    \sum_{x = 0}^1Pr(x|\mu) &= 1\\
    \mathbb{E}[x] &= \mu\\
    \mathrm{var}[x] &= \mu(1 - \mu)
  \end{align}
  Show that the entropy of a Bernoulli distributed random variable is
  given by
  \begin{align}
    \mathrm{H}[x] = -\mu\ln\mu - (1 - \mu)\ln(1 - \mu)
  \end{align}
\end{prob}
First property
\begin{align}
  \sum_{x = 0}^1Pr(x|\mu) &= Pr(x = 0|\mu) + Pr(x = 1|\mu)
  = 1 - \mu + \mu = 1
\end{align}
Second property
\begin{align}
  \mathbb{E}[x] = \sum_{x = 0}^1xPr(x) = 0\cdot(1 - \mu) + 1\cdot\mu = \mu
\end{align}
Third property
\begin{align}
  \mathrm{var}[x] &= E(x^2 + E(x)^2 - 2xE(x))\\
  &= E(x^2) + E(E(x)^2) - E(2xE(x))\\
  &= E(x^2) + E(E(x)^2) - 2E(xE(x))\\
  &= E(x^2) + E(x)E(E(x)) - 2E(x)E(x)
  &= E(x^2) - E(x)E(x)\\
  &= \mu - \mu^2 = \mu(1 - \mu)
\end{align}
The entropy of a discreete random variable is given by
\begin{equation}
  \mathrm{H}[x] = -\sum_xPr(x)\ln\,Pr(x)
\end{equation}
So we just plugin the two x values into the sum
\begin{equation}
  -Pr(0)\ln\,Pr(0) - Pr(1)\ln\,Pr(1)
\end{equation}
Yielding
\begin{equation}
  -(1 - \mu)\ln(1 - \mu) - \mu\ln\mu
\end{equation}

\section{Introduction to Statistical Learning}

\subsection{Chapter 2.4}

\subsubsection{Exercise 1}

\paragraph{a)}

A flexible method is better because the large sample size protects
against overfitting.

\paragraph{b)}

Creating a model for this scenario is very hard because of the large
number of predictors. Again I believe an inflexible method would be
most appropriate because of overfitting risks.

\paragraph{c)}

A flexible method must be used to accurately predict non-linear
relationships.

\paragraph{d)}

An inflexible method is preferrable. A flexible one would be
\textit{fooled by randomness} and increase variance.

\subsubsection{Exercise 2}

\paragraph{a)}

Salary is best modelled as a non-discreete variable and we are
therefore dealing with a regression problem. We want to understand the
relationship between variables and it is therefore an inference
problem.

\paragraph{b)}

It is a classification problem because outcomes are classified into
\textit{success} or \textit{failure}. It is mostly about prediction
because we want to estimate the likelihood of a future event. It might
also be interesting to understand what variables affect whether a
product succeeds or not. Therefore it can also be seen as an inference
problem.

\paragraph{c)}

This is a pure prediction problem as we are unlikely to be able to
understand what factors affect the \% change in the USD/Euro exchange
rate. It is also a regression problem as \% change is a continuous
variable.

\subsubsection{Exercise 7}

\paragraph{a)}

Distances are 3.0, 2.0, 3.16, 2.24, 1.41 and 1.74.

\paragraph{b)}

With $K = 1$ class is \textit{green}.

\paragraph{c)}

With $K = 3$ class is drawn from points $\{2, 5, 6\}$ with classes
$\{red, green, red\}$ and is therefore \textit{red}.

\paragraph{d)}

If the Bayes decision boundary is highly non-linear, then the
\textit{best} value for $K$ would be small. The larger the value for
$K$, the smoother the decision boundary.

\section{Computer vision: models, learning and inference}

\subsection{Chapter 2}

\begin{prob}{2.1}
  Give a real-world example of a joint distribution $Pr(x, y)$ where
  $x$ is discrete and $y$ is continuous.
\end{prob}
The $x$ variable can represent height in centimeters and $y$ European shoe size.

\begin{prob}{2.2}
  What remains if I marginalize a joint distribution $Pr(v, w, x, y, z)$ over five
  variables with respect to variables $w$ and $y$? What remains if I marginalize the resulting
  distribution with respect to $v$?
\end{prob}
Marginalization over the variables $w$ and $y$ results in the joint
distribution $Pr(v, x, z)$. Marginalizing the resulting distribution
over $v$ results in $Pr(x, z)$.
\begin{prob}{2.3}
  Show that the following relation is true:
  \begin{equation}
    Pr(w, x, y, z) = Pr(x, y)Pr(z|w, x, y)Pr(w|x, y)
  \end{equation}
\end{prob}
Expansion of the L.H.S:
\begin{align}
  Pr(w, x, y, z) &= Pr(z|w,x,y)Pr(w,x,y)\\ &= Pr(z|w,x,y)Pr(w|x,y)Pr(x,y)
\end{align}
\begin{prob}{2.4}
  In my pocket there are two coins. Coin 1 is unbiased, so the
  likelihood $Pr(h = 1|c = 1)$ of getting heads is 0.5 and the
  likelihood $Pr(h = 0|c = 1)$ of getting tails is also 0.5. Coin 2 is
  biased, so the likelihood $Pr(h = 1|c = 2)$ of getting heads is 0.8
  and the likelihood $Pr(h = 0|c = 2)$ of getting tails is 0.2. I
  reach into my pocket and draw one of the coins at random. There is
  an equal prior probability I might have picked either coin. I flip
  the coin and observe a head. Use Bayes' rule to compute the
  posterior probability that I chose coin 2.
\end{prob}
The posterior probability is expressed as $Pr(y|x)$, the probability
of $y$ given the evidence $x$. In this case, we are calculating $Pr(c
= 2|h = 1)$, the probability that coin 2 was choosen given that we
observed a heads. We apply Bayes' rule with $y$ set to $c = 2$ and $x$
set to $h = 1$
\begin{align}
  Pr(c = 2|h = 1) &= \frac{Pr(h=1|c=2)Pr(c=2)}{Pr(h=1)}\\
  &=\frac{0.8\cdot0.5}{Pr(h=1)}
\end{align}
The exercise has already given us the probabilities for $Pr(h=1|c=2)$
and $Pr(c=2)$, so what is left is calculating $Pr(h=1)$ by
marginalizing with respect to $c$:
\begin{align}
  Pr(h=1) &= \sum^2_{i=1}Pr(h=1, c=i)\\
  &= Pr(h=1,c=1) + Pr(h=1,c=2)\\
  &= Pr(h=1|c=1)Pr(c=1) + Pr(h=1|c=2)Pr(c=2)\\
  &= 0.5\cdot0.5 + 0.8\cdot0.5\\
  &= 0.65
\end{align}
The value inserted in the original equation results in the answer about 62 \%.
\begin{prob}{2.5}
  If variables $x$ and $y$ are independent and variables $x$ and $z$ are
  independent, does it follow that variables $y$ and $z$ are independent?
\end{prob}
No. Let $x$ and $y$ be the result of fair coin tosses, with heads
represented as 1 and tails as 0. Then $x$ and $y$ are independent
variables. Let $z$ be defined as $z = 3y$. Then $x$ and $z$ are
independent, but $y$ and $z$ are very much dependent.

\begin{prob}{2.6}
  Use the following equation
  \begin{equation}
    Pr(x|y = y^*) = \frac{Pr(x,y=y^*)}{\int\,Pr(x,y=y^*)dx}
    = \frac{Pr(x, y=y^*)}{Pr(y=y^*)}
  \end{equation}
  to show that when $x$ and $y$ are independent, the
  marginal distribution $Pr(x)$ is the same as the conditional
  distribution $Pr(x|y = y^*)$ for any $y^*$.
\end{prob}
For independent variables we have $Pr(x,y) = Pr(x)Pr(y)$ therefore:
\begin{align}
  Pr(x|y = y^*) = \frac{Pr(x, y=y^*)}{Pr(y=y^*)} = \frac{Pr(x)Pr(y=y^*)}{Pr(y=y^*)} = Pr(x)
\end{align}
\begin{prob}{2.7}
  The joint probability $Pr(w, x, y, z)$ over four variables factorizes as
  \begin{equation}
    Pr(w, x, y, z) = Pr(w)Pr(z|y)Pr(y|x, w)Pr(x)
  \end{equation}
  Demonstrate that $x$ is independent of $w$ by showing that $Pr(x, w)
  = Pr(x)Pr(w)$.
\end{prob}
Ask for help.
\begin{prob}{2.7}
  Consider a biased die where the probabilities of rolling sides {1,
    2, 3, 4, 5, 6} are {1/12, 1/12, 1/12, 1/12, 1/6, 1/2},
  respectively. What is the expected value of the die?  If I roll the
  die twice, what is the expected value of the sum of the two rolls?
\end{prob}
The expected value of a function of a discreete random variable is defined as
\begin{equation}
  E(f(x)) = \sum_xf(x)Pr(x)
\end{equation}
, so we simply plug in the numbers
\begin{align}
  E(f(x)) &= 1\cdot\tfrac{1}{12} + 2\cdot\tfrac{1}{12} + 3\cdot\tfrac{1}{12} + 4\cdot\tfrac{1}{12} + 5\cdot\tfrac{1}{6} + 6\cdot\tfrac{1}{2}\\
  &= 10\cdot\tfrac{1}{12} + 5\cdot\tfrac{1}{6} + 6\cdot\tfrac{1}{2} = \tfrac{20}{12} + \tfrac{36}{12} = \tfrac{56}{12} = \tfrac{14}{3}
\end{align}
\begin{prob}{2.9}
  Prove the four relations for manipulating expectations.
  \begin{align}
    E(k) &= k\\
    E(kf(x)) &= kE(f(x))\\
    E(f(x) + g(x)) &= E(f(x)) + E(g(x))\\
    E(f(x)g(y)) &= E(f(x))E(g(y))
  \end{align}
  for the last case, it is assumed that $x$ and $y$ are independent so
  you will need to use the definition of independence.
\end{prob}
We prove the first relation using a continuous random variable
\begin{align}
  E(k) = \int\,kPr(x)\,dx = k\int\,Pr(x)\,dx=k
\end{align}
since we know that integrating over a continuous variable always
yields one. For the second relation
\begin{align}
  E(kf(x)) = \int\,kf(x)Pr(x)\,dx = k\int\,f(x)Pr(x)\,dx = kE(f(x))
\end{align}
The third relation
\begin{align}
  E(f(x) + g(x)) &= \int\,(f(x) + g(x))Pr(x)\,dx\\
  &= \int\,f(x)Pr(x)\,dx + \int\,g(x)Pr(x)\,dx\\
  &= E(f(x)) + E(g(x))
\end{align}
The definition of independence is
\begin{equation}
  Pr(x, y)=Pr(x|y)Pr(y) = Pr(x)Pr(y)
\end{equation}
giving us
\begin{align}
  E(f(x)g(y)) &= \iint\,f(x)g(y)Pr(x,y)\,dx\,dy \\
  &= \iint\,f(x)g(y)Pr(x)Pr(y)\,dx\,dy
\end{align}
The integrand is fortunately separable
\begin{align}
  \iint\,f(x)g(y)Pr(x)Pr(y)\,dx\,dy &= \int\,f(x)Pr(x)\,dx\cdot\int\,g(y)Pr(y)\,dy\\
\end{align}
Which is the same as $E(f(x))E(g(y))$.
\begin{prob}{2.10}
  Use the relations from problem 2.9 to prove the following
  relationship between the second moment around zero and the second
  moment about the mean (variance):
  \begin{equation}
    E((x - \mu)^2) = E(x^2) - E(x)^2
  \end{equation}
\end{prob}
Once one realises that $\mu = E(x)$, the solution becomes trivial
\begin{equation}
  E(x^2 + E(x)^2 - 2xE(x))
\end{equation}
Using the rules of addition yields
\begin{equation}
  E(x^2) + E(E(x)^2) - E(2xE(x))
\end{equation}
Rule of homogenity, applied to the last term
\begin{equation}
  E(x^2) + E(E(x)^2) - 2E(xE(x))
\end{equation}
We can factor out one $E(x)$ from each of the last two terms since
$E(x)$ itself is a constant
\begin{equation}
  E(x^2) + E(x)E(E(x)) - 2E(x)E(x)
\end{equation}
Because $E(x)$ is a constant, $E(E(x)) = E(x)$
\begin{equation}
  E(x^2) + E(x)E(x) - 2E(x)E(x) = E(x^2) - E(x)^2
\end{equation}
The proof is done.
\subsection{Chapter 3}
\begin{prob}{3.1}
  Consider a variable $x$ which is Bernoulli distributed with
  parameter $\lambda$. Show that the mean $E(x)$ is $\lambda$ and the
  variance $E((x - E(x))^2)$ is $\lambda(1 - \lambda)$.
\end{prob}
We begin by showing the mean
\begin{align}
  E(x) = \sum_{x=0}^1f(x)P(x) = 0\cdot(1-\lambda) + 1\cdot\lambda = \lambda
\end{align}
and then the variance using the formula $Var(x) = E(x^2) - E(x)^2$
\begin{align}
  E((x - E(x))^2) &= E(x^2) - E(x)^2
\end{align}
first term
\begin{align}
  E(x^2) = \sum_{x=0}^1f(x^2)P(x^2) = 0\cdot(1-\lambda) + 1\cdot\lambda = \lambda
\end{align}
second term
\begin{align}
  E(x)^2 = E(x)E(x) = \lambda^2
\end{align}
giving us the result $\lambda - \lambda^2$. A smarter method, using
the identities $E(x) = E(x^2) = \lambda$ and $E(k) = k$
\begin{align}
  E((x-E(x))^2) &= E(x^2 + E(x)^2 - 2xE(x))\\
  &= E(x^2) + E(x)^2 - E(2xE(x))\\
  &= \lambda + \lambda^2 - E(2x\lambda)\\
  &= \lambda + \lambda^2 - 2\lambda\lambda\\
  &= \lambda - \lambda^2 = \lambda(1-\lambda)
\end{align}
\begin{prob}{3.2}
  Calculate an expression for the mode (position of the peak) of the
  beta distribution with $\alpha, \beta > 1$ in terms of the
  parameters $\alpha$ and $\beta$.
\end{prob}
We seek the point at which the pdf of the beta distribution is at its
greatest
\begin{equation}
  Pr(x) = \frac{1}{B(\alpha, \beta)}x^{\alpha - 1}(1 - x)^{\beta - 1}
\end{equation}
We derive it with respect to $x$. Because the beta function is
constant we can just skip it
\begin{equation}
  \frac{d}{dx}\left(x^{\alpha - 1}(1 - x)^{\beta - 1}\right)
\end{equation}
Yielding the derivative
\begin{equation}
  (\alpha-1)x^{\alpha - 2}(1-x)^{\beta - 1} - x^{\alpha - 1}(\beta - 1)(1 - x)^{\beta - 1}
\end{equation}
To find the maximum, we just set it equal to 0 and calculate. Resulting in
\begin{equation}
  (\alpha - 1)(1 - x) = (\beta - 1)x
\end{equation}
Some more algebraic manipulation yields
\begin{equation}
  x = \frac{\alpha - 1}{\beta + \alpha - 2}
\end{equation}
\section{Exams}

\subsection{Exam 2017-10-21}

\begin{prob}{A-1}
  What is the goal of \textit{maximum a posteriori estimation}?
\end{prob}
The goal of maximum a posteriori estimation is to optimize the
likelihood of the new observation in conjunction with a priori
information.

\begin{prob}{A-2}
  What is the underlying assumption unique to a \textit{naive Bayes
    classifier}?
\end{prob}
The underlying assumption is that all features are regarded as
conditionally independent. The assumption can be expressed as
\begin{equation}
  P(Y = y, X = x) = P(X = x)P(Y = y)
\end{equation}
for any pair of features $X, Y$.
\begin{prob}{A-3}
  Consider a single toss of a fair coin. Regarding the uncertainty of
  the outcome \{head, tail\}.
\end{prob}
One bit of data is enough to encode the outcome. Therefore the entropy
is equal to one bit.
\begin{prob}{A-4}
  Which regression methods includes an additional shrinkage term?
\end{prob}
The two best known are ridge regression and the lasso.
\begin{prob}{A-5}
  What happens during \textit{training} in an artificial neural
  network?
\end{prob}
Weights are adjusted to minimize the output error.
\begin{prob}{A-6}
  What is the consequence of using a kernel-function in a support
  vector machine?
\end{prob}
Classification takes place in a ``virtual'' high-dimensional space.
\begin{prob}{A-7}
  What is one characteristic of Ensemble methods in machine learning?
\end{prob}
Weak models are trained and combined.
\begin{prob}{A-8}
  What is the main role of the principal component analysis (PCA) in
  the \textit{Subspace Method} for classification?
\end{prob}
To compute a subspace that represents the training data distribution
in each class.
\begin{prob}{B-1}
  Describe the following terms used in machine learning.
\end{prob}
\begin{description}[style=nextline]
\item[Random forests]
  An ensemble of decision trees.
\item[RANSAC]
  Robust method to fit a model to data with outliers.
\item[Dropout]
  An approach to train artificial neural networks.
\item[$k$-means]
  Clustering method based on centroids.
\item[Curse of dimensionality]
  Issues in data sparsity space.
\item[Gini impurity] A measure of how often a randomly choosen element
  from the set would be incorrectly labeled if it was randomly labeled
  according to the distribution of labels in the subset. It can be
  used as a measure of information gain or predictability when
  creating a decision tree.
\item[Expected maximization]
  An iterative method to find maximum likelihood or maximum a
  posteriori estimates where the model depends on unobserved latent
  variables.
\item[Projection length]
  Similarity measure in the subspace method.
\end{description}
\begin{prob}{B-2}
  Suppose you need to design an identity verification system based on
  face recognition whose goal is to confirm or reject the identity
  claimed by each users. The system is only supposed to work with a
  close set of $N > 1$ individuals. Users are assumed to claim any of
  the $N$ identities uniformly at random. Call $\alpha$ the
  probability of false acceptance and $\beta$ the probability of false
  rejection of the system.

  \textbf{a)} What is the a priori probability that the claimed
  identity is correct?

  \textbf{b)} What are the conditions on $\alpha$ and $\beta$ to make
  sure that the claimed identity is more likely to be correct if the
  system accepts the user and more likely to be incorrect if the
  system rejects the user?

  \textbf{c)} What are the conditions on $\alpha$ and $\beta$ from the
  previous point if you assume equal error rates?
\end{prob}
Call $C$ the event that the claimed identity matches the true
identity. A priori, the probability that the claimed identity is
correct is $Pr(C) = \frac{1}{N}$ and $Pr(\neg\,C) = 1 - Pr(C)$, where
$N$ is the number of individuals.

Let $V$ be the event that the system recognizes the identity, and
$\neg\,V$ that it rejects it. Then we can specify the false positives
and true negatives as follows
\begin{align}
  &P(V|\neg\,C) = \alpha \quad &P(\neg\,V|\neg\,C) = 1 - \alpha\\
  &P(\neg\,V|C) = \beta \quad &P(V|C) = 1 - \beta
\end{align}
We want to ensure two things. First, that the probability that the
identity is correct given that the system valides it, is higher than
if it rejects it
\begin{equation}
  P(C|V) > P(\neg\,C|V)
\end{equation}
Using Bayes' rule, we can express the inequality with the
probabilities given
\begin{equation}
  Pr(C|V) = \frac{Pr(V|C)Pr(C)}{Pr(V)}
\end{equation}
and
\begin{equation}
  Pr(\neg\,C|V) = \frac{Pr(V|\neg\,C)Pr(\neg\,C)}{Pr(V)}
\end{equation}
Factoring out the denominators and inserting in the original
inequality yields
\begin{equation}
  Pr(V|C)Pr(C) > Pr(V|\neg\,C)Pr(\neg\,C)
\end{equation}
Then we resolve the probability functions
\begin{equation}
  (1-\beta)\frac{1}{N} > \alpha\left(1-\frac{1}{N}\right)
\end{equation}
After a bit of substituting and rearranging, we get
\begin{equation}
  (N - 1)\alpha + \beta < 1
\end{equation}
We perform the same process with the second constraint, that the
probability that the identity is incorrect \textit{given} that the
system rejects it, is higher than if it validates it.
\begin{equation}
  P(\neg\,C|\neg\,V) > P(C|\neg\,V)
\end{equation}
Bayes' rule yields
\begin{equation}
  Pr(\neg\,V|\neg\,C)Pr(\neg\,C) > Pr(\neg\,V|C)Pr(C)
\end{equation}
Insertion of probabilities
\begin{equation}
  (1-\alpha)\left(1 - \frac{1}{N}\right) < \beta\frac{1}{N}
\end{equation}
After a bit of algebraic manipulation, we arrive at
\begin{equation}
  (N - 1)\alpha + \beta < N - 1
\end{equation}
Because $N - 1 \geq 1$, the only constraint we have is $(N - 1)\alpha
+ \beta < 1$.

Equal error rates means that $\alpha = \beta$ giving us the inequality
\begin{equation}
  \alpha = \beta < \frac{1}{N}
\end{equation}

\begin{prob}{B-3}
  Figure 1 shows five data points on the real number line (x-axis)
  denoted by the symbol $\times$ and two probability distribution
  functions (PDFs) denoted by $d_1$ (continuous line) and $d_2$
  (dashed line).

  \textbf{a)} Which PDF fits the data set best according to the
  likelihood criterion, assuming that the data is i.i.d.?

  \textbf{b)} Assuming the shapes are all triangular and symmetric
  around the center, how many parameters do you need to define $d_1$
  and $d_2$ respectively? \textbf{Note:} $d_2$ can be considered as a
  mixture of distributions in the form of $d_1$.
\end{prob}
The likelihood $\mathcal{L}(\mathcal{D})$ is an indication of how well
a probability distribution (model) fits the data. If the data points
are i.i.d., the likelihood of the data can be expressed as the product
of the likelihood of the single points:
\begin{equation}
  \mathcal{L}_{d_j}(\mathcal{D}) = \prod_iPr(x_i|d_j)
\end{equation}
where $\mathcal{D}$ is the available data and $d_j$ a
distribution. The likelihood of the set of points given $d_2$ can be
calculated easily as
\begin{align}
  \mathcal{L}_{d_2}(\mathcal{D}) &= \prod_iPr(x_i|d_2)\\
  &= Pr(-4)Pr(-2.5)Pr(0)Pr(1.5)Pr(4)\\
  &= 0.2^5 = 0.00032
\end{align}
and for $d_1$
\begin{align}
  \mathcal{L}_{d_1}(\mathcal{D}) &= \prod_iPr(x_i|d_1)\\
  &= Pr(-4)Pr(-2.5)Pr(0)Pr(1.5)Pr(4)\\
  &= (0.2\cdot0.2)(0.5\cdot0.2)(1.0\cdot0.2)(0.7\cdot0.2)(0.2\cdot0.2)\\
  &= 0.2^5(0.2\cdot0.5\cdot1.0\cdot0.7\cdot0.2)\\
  &= 0.00000448
\end{align}
We see that $d_2$ fits the data best.

To define the distribution $d_1$, we need just two parameters; $\mu$
defining the center of the distribution (which coincides with the
expected value and the mode of it), and a parameter related to
spread. For simplicity, the second parameter could be the value of the
PDF at the center: $\delta = Pr(\mu|d_1)$. Because the area below the
distribution must equal to 1, the distribution is fully defined if we
know $\mu$ and $\delta$.

If we consider $d_2$ as a mixture of $N$ distributions in the form of
$d_1$, we need $N\cdot2$ parameters (from $d_1$), plus $N$
weights. Because the weights must sum to 1, only $N - 1$ weights are
free parameters. In total we have $N\cdot2 + N - 1 = N\cdot3 - 1$
parameters. From the figure, we see that $d_2$ is comprised of 10
components, so the number of parameters would be 29 at most.

\end{document}






\end{document}
